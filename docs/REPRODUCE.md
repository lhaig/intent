# Reproducing the Intent Compiler

This document describes how to reproduce the Intent compiler from scratch using an AI code assistant, following the same prompts and process used to build the original.

The goal is to test whether different AI assistants (or the same ones at different times) produce a working compiler from the same instructions.

## What you need

- **Go 1.21+** (the compiler is pure Go with zero dependencies)
- **An AI code assistant** (Claude Code, Cursor, Copilot, Windsurf, Aider, etc.)
- **~2-4 hours** depending on your assistant and how many iterations you need
- **Rust** (optional, only needed to test native binary output)
- **Z3** (optional, only needed to test contract verification)
- **Node.js** (optional, for testing JS and WASM targets)

## The experiment

The Intent compiler was built entirely through AI-assisted development. Every line of code was generated by an AI assistant working from design documents and prompts. The question is: **can someone else reproduce a working compiler from the same starting point?**

We're not looking for byte-identical output. We're looking for:
- Does it compile Intent source files?
- Do the contracts enforce correctly?
- Does multi-target output work?
- How many iterations did it take?
- What went wrong along the way?

## Step-by-step instructions

### Phase 1: Bootstrap (the design documents)

Start by giving your AI assistant the design document and grammar. These are the "spec" the compiler was built from.

1. Create a new Go module: `go mod init github.com/yourusername/intent`
2. Feed your assistant [docs/DESIGN.md](DESIGN.md) and [docs/grammar.ebnf](grammar.ebnf) from this repo
3. Give this prompt:

> Build a compiler for the Intent programming language as specified in the design document and grammar. The compiler should be written in Go with zero external dependencies. Start with the lexer and parser, then add semantic checking, then Rust code generation. The CLI should be called `intentc` with subcommands: `build`, `check`, `fmt`, `lint`.

### Phase 2: Core language

Once you have a basic parser, work through these features in order. Each prompt builds on the previous:

**Prompt 2a - Lexer and Parser:**
> Implement a lexer that tokenizes all Intent keywords, operators, and literals as defined in the grammar. Then implement a recursive-descent parser that produces an AST. Include the module declaration, functions with contracts (requires/ensures), entities with fields/invariants/constructor/methods, and entry functions.

**Prompt 2b - Type checker:**
> Add semantic analysis that type-checks the AST. Check: variable declarations and usage, function parameter/return types, entity field types, contract expression types (must be Bool), method calls on entities, constructor calls. Report errors with line numbers.

**Prompt 2c - Rust code generation:**
> Generate Rust source code from the checked AST. Map: Int->i64, Float->f64, String->String, Bool->bool. Generate `assert!()` for requires/ensures contracts. Generate `__check_invariants()` methods for entity invariants. Generate `fn main()` wrapper for entry functions. The output should compile with `rustc` or `cargo`.

**Prompt 2d - CLI:**
> Create `cmd/intentc/main.go` with subcommands: `build` (compile to Rust, then invoke cargo), `check` (parse + type-check only), `fmt` (format source code), `lint` (style warnings).

### Phase 3: Extended features

Add these one at a time, testing after each:

**Prompt 3a:** Add while loops with break/continue, and for-in loops over arrays and integer ranges.

**Prompt 3b:** Add `Array<T>` with index access, `len()`, `push()`. Add `print()` as a built-in.

**Prompt 3c:** Add enums with unit and data-carrying variants. Add pattern matching with exhaustiveness checking.

**Prompt 3d:** Add `Result<T,E>` and `Option<T>` built-in types. Add the try operator (`?`).

**Prompt 3e:** Add `import` declarations for multi-file projects. Add `public`/private visibility. Add cross-file type checking.

### Phase 4: IR and multi-target

**Prompt 4a:** Introduce an intermediate representation (IR) between the checker and code generation. The IR should have typed expression nodes, resolved call targets, and explicit old() captures for ensures clauses.

**Prompt 4b:** Refactor the Rust backend to consume IR instead of AST. Add a JavaScript backend that generates ES6 classes and template literals.

**Prompt 4c:** Add a WASM backend that emits WASM binary format directly from IR (no Rust intermediary).

### Phase 5: Verification

**Prompt 5a:** Add Z3 SMT solver integration. Translate contracts to SMT-LIB format. Add `intentc verify` command that reports per-contract verification status.

### Validation

After each phase, run:
```bash
go test ./...
go vet ./...
gofmt -l .
```

Test against the example files in this repo's `examples/` directory. A working compiler should handle at least:
- `examples/hello.intent` - basic entry function
- `examples/fibonacci.intent` - recursion, loops, contracts
- `examples/bank_account.intent` - entities, invariants, old() expressions
- `examples/enum_basic.intent` - enums, pattern matching

## Reporting results

Please open a GitHub issue with the label `reproduction` and include:

**Title:** `[Reproduction] <your-assistant> - <outcome>`

Example: `[Reproduction] Cursor + Claude Sonnet - Phases 1-3 working, Phase 4 partial`

**In the issue, include:**

1. **What AI assistant did you use?** (tool + model, e.g. "Claude Code with Opus", "Cursor with GPT-4o")
2. **How far did you get?** (which phases completed successfully)
3. **What worked well?** (features that compiled and passed tests on first try)
4. **What needed extra iterations?** (features that required debugging or re-prompting)
5. **What failed?** (features you couldn't get working)
6. **Approximate time spent** (wall clock, not just AI time)
7. **Number of prompts/iterations** (rough count)
8. **Any interesting observations** (surprising successes, common failure modes, etc.)

Optional but appreciated:
- Link to your repo (if public)
- The exact prompts you used (if they differed from the ones above)
- Test output showing which examples pass

## What we're trying to learn

- **Reproducibility:** Can different AI assistants build a working compiler from the same spec?
- **Failure modes:** Where do AI assistants consistently struggle? (parser edge cases? type checker? code generation?)
- **Iteration count:** How many back-and-forth cycles does it take to get each phase working?
- **Tool differences:** Do some AI assistants handle compiler construction better than others?
- **Spec quality:** Is the design document sufficient, or do people need to add clarifications?

This isn't a competition. Partial results are just as valuable as complete ones. If your assistant gets stuck on Phase 2 and you can describe *why*, that's useful data.
